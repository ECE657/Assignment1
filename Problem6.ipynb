{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from utils.activations import softmax, sigmoid\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "#define constants\n",
    "num_class = 4 # no of classes\n",
    "hidden_layer_units = 25  #number of units in the hidden layer\n",
    "epochs = 150\n",
    "alpha = 0.001 #learning rate \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSource\n",
    "X_train, Y_train is parsed from csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(os.path.join('data','train_data.csv'), header=None).add_prefix('Feature_')\n",
    "y = pd.read_csv(os.path.join('data', 'train_labels.csv'), header=None, names=[\"Label_0\", \"Label_1\", \"Label_2\", \"Label_3\"])\n",
    "X_train = x.values\n",
    "Y_train = y.values\n",
    "m, n = X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer and Forward Propagation\n",
    "We then implement Layer and For-prop as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Layer(A_in, W, B, g):\n",
    "    \"\"\"\n",
    "    :param A_in: shape(m,n) - input data\n",
    "    :param W: shape(feature,units) - weight matrix, n0 feature  x units, \n",
    "    :param b: shape(units,1) - bias vector, n0 units x 1\n",
    "    :param g: activation function(e.g sigmoid, relu, softmax, ...)\n",
    "    :return:\n",
    "    A_out: shape(m, units): output data - m x units\n",
    "    \"\"\"\n",
    "    Z = np.dot(A_in, W) + B\n",
    "    A_out = g(Z)\n",
    "    return Z,A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_prop(x, W1, b1, W2, b2):\n",
    "    z1, a1 = Layer(x, W1, b1, sigmoid)  #hidden layer with sigmoid activation\n",
    "    z2, a2 = Layer(a1, W2, b2, softmax) #output layer with softmax activation\n",
    "    return z1, a1, z2, a2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model parameters\n",
    "The model parameters are intialized randomly as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_model_params():\n",
    "    W1 = np.random.randn(n, hidden_layer_units)\n",
    "    b1 = np.random.randn(hidden_layer_units)\n",
    "    W2 = np.random.randn(hidden_layer_units, num_class)\n",
    "    b2 = np.random.randn(num_class)\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x): \n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def compute_backward_prop(Z1, A1, A2, W2,X, Y): \n",
    "    dz2 = A2 - Y\n",
    "    dW2 = np.dot(A1.T,dz2)\n",
    "    db2 = dz2\n",
    "    dz1 = np.dot(dz2,W2.T)\n",
    "    db1 = dz1 * sigmoid_derivative(Z1)\n",
    "    dW1 = np.dot(X.T,dz1 * sigmoid_derivative(Z1))\n",
    "    return db1, dW1, db2, dW2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha_):\n",
    "    if dW1.shape == (n, hidden_layer_units):\n",
    "        W1 = W1 - alpha_ * dW1\n",
    "    if dW2.shape == (hidden_layer_units, num_class):\n",
    "        W2 = W2 - alpha_ * dW2\n",
    "    if db1.shape == (m, hidden_layer_units):\n",
    "        b1 = b1 - alpha_ * db1.sum(axis=0)\n",
    "    if db2.shape == (m, num_class):\n",
    "        b2 = b2 - alpha_ * db2.sum(axis=0)\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y_hat, y):\n",
    "    return np.mean(np.square(y_hat - y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and accuracy\n",
    "In order to analyze the results, the implementation of prediction and accuracy are described below:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_output_layer(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def predict(X, B1, W1, B2, W2):\n",
    "    Z1 = np.dot(X, W1) + B1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + B2\n",
    "    A2 = softmax(Z2)\n",
    "    prediction = np.argmax(A2, 0)\n",
    "    return prediction\n",
    "\n",
    "def accuracy(Y_hat, Y):\n",
    "    # print(Y_hat, Y)\n",
    "    return np.sum(Y_hat == Y) / Y.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap everything up  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(X, Y, epochs, alpha):\n",
    "    B1, W1, B2, W2 = initialize_model_params()\n",
    "    for i in range(epochs):\n",
    "        z1, a1, z2, a2 = compute_forward_prop(X, W1, B1, W2, B2)\n",
    "        db1, dW1, db2, dW2 = compute_backward_prop(z1, a1, a2, W2, X, Y) \n",
    "        b1, W1, b2, W2 = update_model_params(W1, B1, W2, B2, dW1, db1, dW2, db2, alpha)\n",
    "\n",
    "        cost = compute_cost(a2, Y)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            print(f\"cost = {cost}\")\n",
    "        # print(f\"Accuracy: {accuracy(convert_to_one_hot(a2), Y)}\")\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training process with $epochs=100$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "0.32109911778100236\n",
      "Iteration:  1\n",
      "0.2752278437034565\n",
      "Iteration:  2\n",
      "0.37914989214566885\n",
      "Iteration:  3\n",
      "0.38036276965291066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quanquach/Desktop/ECE657/Assignment1/utils/activations.py:21: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  4\n",
      "0.37616142828442234\n",
      "Iteration:  5\n",
      "0.35840069527685947\n",
      "Iteration:  6\n",
      "0.3654603285201286\n",
      "Iteration:  7\n",
      "0.38036262380675673\n",
      "Iteration:  8\n",
      "0.3742461564300916\n",
      "Iteration:  9\n",
      "0.3636277279110592\n",
      "Iteration:  10\n",
      "0.31072315764626157\n",
      "Iteration:  11\n",
      "0.2406317845753729\n",
      "Iteration:  12\n",
      "0.2668052067825854\n",
      "Iteration:  13\n",
      "0.3176878170303003\n",
      "Iteration:  14\n",
      "0.2558652744210048\n",
      "Iteration:  15\n",
      "0.25776751853551694\n",
      "Iteration:  16\n",
      "0.28244978181975217\n",
      "Iteration:  17\n",
      "0.2994792457295599\n",
      "Iteration:  18\n",
      "0.2486008393950285\n",
      "Iteration:  19\n",
      "0.2759140387739569\n",
      "Iteration:  20\n",
      "0.2066723040385362\n",
      "Iteration:  21\n",
      "0.27822122874207933\n",
      "Iteration:  22\n",
      "0.2416276490954042\n",
      "Iteration:  23\n",
      "0.36311546837731945\n",
      "Iteration:  24\n",
      "0.3457634742428219\n",
      "Iteration:  25\n",
      "0.27702203371190137\n",
      "Iteration:  26\n",
      "0.24708159181845168\n",
      "Iteration:  27\n",
      "0.27502806850224987\n",
      "Iteration:  28\n",
      "0.1373581805882618\n",
      "Iteration:  29\n",
      "0.17364967108947973\n",
      "Iteration:  30\n",
      "0.2270935094381853\n",
      "Iteration:  31\n",
      "0.1366159156549364\n",
      "Iteration:  32\n",
      "0.14253681948955121\n",
      "Iteration:  33\n",
      "0.13631268233334468\n",
      "Iteration:  34\n",
      "0.13737574639491287\n",
      "Iteration:  35\n",
      "0.13620996860265627\n",
      "Iteration:  36\n",
      "0.13649973816033484\n",
      "Iteration:  37\n",
      "0.13705509415000922\n",
      "Iteration:  38\n",
      "0.13774718414173173\n",
      "Iteration:  39\n",
      "0.12791380837180566\n",
      "Iteration:  40\n",
      "0.13573579851220963\n",
      "Iteration:  41\n",
      "0.1322137119831534\n",
      "Iteration:  42\n",
      "0.1371920813755891\n",
      "Iteration:  43\n",
      "0.12419144331432734\n",
      "Iteration:  44\n",
      "0.1389936743856952\n",
      "Iteration:  45\n",
      "0.12632086513415655\n",
      "Iteration:  46\n",
      "0.1322828691363755\n",
      "Iteration:  47\n",
      "0.13112744012158165\n",
      "Iteration:  48\n",
      "0.13937259681334746\n",
      "Iteration:  49\n",
      "0.12356140133449864\n",
      "Iteration:  50\n",
      "0.12683343839472236\n",
      "Iteration:  51\n",
      "0.13124582797911144\n",
      "Iteration:  52\n",
      "0.13094964271253856\n",
      "Iteration:  53\n",
      "0.11988780168701017\n",
      "Iteration:  54\n",
      "0.072745203936339\n",
      "Iteration:  55\n",
      "0.08752754048826085\n",
      "Iteration:  56\n",
      "0.16151308510321233\n",
      "Iteration:  57\n",
      "0.12532742156879734\n",
      "Iteration:  58\n",
      "0.04555724132905933\n",
      "Iteration:  59\n",
      "0.03715806257759351\n",
      "Iteration:  60\n",
      "0.03765941928172257\n",
      "Iteration:  61\n",
      "0.034963024141880045\n",
      "Iteration:  62\n",
      "0.03481078662827704\n",
      "Iteration:  63\n",
      "0.03361795448704494\n",
      "Iteration:  64\n",
      "0.035234817307305856\n",
      "Iteration:  65\n",
      "0.042658695555665\n",
      "Iteration:  66\n",
      "0.09154407392091304\n",
      "Iteration:  67\n",
      "0.24959186732016916\n",
      "Iteration:  68\n",
      "0.10701780958854214\n",
      "Iteration:  69\n",
      "0.12192227753763779\n",
      "Iteration:  70\n",
      "0.0539790187444733\n",
      "Iteration:  71\n",
      "0.05268719558126454\n",
      "Iteration:  72\n",
      "0.024958325863436586\n",
      "Iteration:  73\n",
      "0.025759937328881422\n",
      "Iteration:  74\n",
      "0.02381422088755454\n",
      "Iteration:  75\n",
      "0.02328105552180218\n",
      "Iteration:  76\n",
      "0.022720685048058836\n",
      "Iteration:  77\n",
      "0.022043312838632298\n",
      "Iteration:  78\n",
      "0.021488240649410142\n",
      "Iteration:  79\n",
      "0.02075653551479907\n",
      "Iteration:  80\n",
      "0.020498408610511335\n",
      "Iteration:  81\n",
      "0.019883935388999317\n",
      "Iteration:  82\n",
      "0.019652469560565374\n",
      "Iteration:  83\n",
      "0.01894481797715668\n",
      "Iteration:  84\n",
      "0.018815457404471297\n",
      "Iteration:  85\n",
      "0.018593170429925275\n",
      "Iteration:  86\n",
      "0.01812676571727346\n",
      "Iteration:  87\n",
      "0.017930916479840072\n",
      "Iteration:  88\n",
      "0.017746271606810193\n",
      "Iteration:  89\n",
      "0.017779913053668675\n",
      "Iteration:  90\n",
      "0.01740567901018874\n",
      "Iteration:  91\n",
      "0.017720193119426565\n",
      "Iteration:  92\n",
      "0.01733877999554978\n",
      "Iteration:  93\n",
      "0.01769760144395822\n",
      "Iteration:  94\n",
      "0.017528047930887813\n",
      "Iteration:  95\n",
      "0.018271598357814677\n",
      "Iteration:  96\n",
      "0.01800979658009814\n",
      "Iteration:  97\n",
      "0.01871760141256014\n",
      "Iteration:  98\n",
      "0.01791013944754017\n",
      "Iteration:  99\n",
      "0.01839345688498198\n",
      "Iteration:  100\n",
      "0.01734219253874814\n",
      "Iteration:  101\n",
      "0.01770937414609539\n",
      "Iteration:  102\n",
      "0.01689427396580729\n",
      "Iteration:  103\n",
      "0.01707659778260101\n",
      "Iteration:  104\n",
      "0.016640808041070352\n",
      "Iteration:  105\n",
      "0.01725930089272279\n",
      "Iteration:  106\n",
      "0.016724660746526038\n",
      "Iteration:  107\n",
      "0.017053655092859762\n",
      "Iteration:  108\n",
      "0.016777107342413538\n",
      "Iteration:  109\n",
      "0.017357835224247685\n",
      "Iteration:  110\n",
      "0.016749390405450773\n",
      "Iteration:  111\n",
      "0.01744220156697328\n",
      "Iteration:  112\n",
      "0.016632280654640917\n",
      "Iteration:  113\n",
      "0.01713313898869714\n",
      "Iteration:  114\n",
      "0.016380477617007744\n",
      "Iteration:  115\n",
      "0.01673318692350936\n",
      "Iteration:  116\n",
      "0.01613269380446202\n",
      "Iteration:  117\n",
      "0.01648266762724255\n",
      "Iteration:  118\n",
      "0.016027737963750597\n",
      "Iteration:  119\n",
      "0.016536747597232396\n",
      "Iteration:  120\n",
      "0.016053895169745907\n",
      "Iteration:  121\n",
      "0.016796374546209327\n",
      "Iteration:  122\n",
      "0.01614408660883389\n",
      "Iteration:  123\n",
      "0.01686938594078974\n",
      "Iteration:  124\n",
      "0.016129543195827063\n",
      "Iteration:  125\n",
      "0.016750321135542656\n",
      "Iteration:  126\n",
      "0.01596412495432844\n",
      "Iteration:  127\n",
      "0.01646407287065217\n",
      "Iteration:  128\n",
      "0.015726497564389066\n",
      "Iteration:  129\n",
      "0.01615483822228279\n",
      "Iteration:  130\n",
      "0.01563906796171409\n",
      "Iteration:  131\n",
      "0.016156301558872984\n",
      "Iteration:  132\n",
      "0.015592115840304501\n",
      "Iteration:  133\n",
      "0.01601662284746855\n",
      "Iteration:  134\n",
      "0.015615817035634886\n",
      "Iteration:  135\n",
      "0.01619664529272432\n",
      "Iteration:  136\n",
      "0.015604737483767393\n",
      "Iteration:  137\n",
      "0.01601629388344849\n",
      "Iteration:  138\n",
      "0.015622900463148298\n",
      "Iteration:  139\n",
      "0.016311659348674575\n",
      "Iteration:  140\n",
      "0.015563200509873155\n",
      "Iteration:  141\n",
      "0.01595104390572319\n",
      "Iteration:  142\n",
      "0.015503582535140621\n",
      "Iteration:  143\n",
      "0.016187459449302635\n",
      "Iteration:  144\n",
      "0.015384509373553361\n",
      "Iteration:  145\n",
      "0.01582368727753877\n",
      "Iteration:  146\n",
      "0.015367074429686087\n",
      "Iteration:  147\n",
      "0.01592546238581212\n",
      "Iteration:  148\n",
      "0.015210641477089665\n",
      "Iteration:  149\n",
      "0.01557988080480563\n"
     ]
    }
   ],
   "source": [
    "b1, W1, b2, W2 = training_data(X_train, Y_train, epochs, alpha)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the proper arguments for out the model. Let's try some unseen tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = np.random.rand(1, n)\n",
    "# prediction = predict(X_test, b1, W1, b2, W2)\n",
    "# print(f\"prediction = {prediction}\")\n",
    "# print(f\"W1 = {W1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
