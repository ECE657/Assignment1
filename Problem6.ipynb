{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from utils.activations import softmax, sigmoid\n",
    "from utils.data_parser import data_split_train_test\n",
    "np.random.seed(1)\n",
    "\n",
    "#define constants\n",
    "num_class = 4 # no of classes\n",
    "hidden_layer_units = 25  #number of units in the hidden layer\n",
    "epochs = 200  #number of iterations\n",
    "alpha = 0.001 #learning rate \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSource\n",
    "X_train, Y_train is parsed from csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(os.path.join('data','train_data.csv'), header=None).add_prefix('Feature_')\n",
    "y = pd.read_csv(os.path.join('data', 'train_labels.csv'), header=None, names=[\"Label_0\", \"Label_1\", \"Label_2\", \"Label_3\"])\n",
    "X, Y, X_v, Y_v = data_split_train_test(x, y)\n",
    "X_train = X.values   #training samples\n",
    "Y_train = Y.values   #training labels\n",
    "\n",
    "m, n = X_train.shape  #samples x features\n",
    "X_Validate = X_v.values  #validation samples\n",
    "Y_Validate = Y_v.values  #validation labels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer and Forward Propagation\n",
    "We then implement Layer and For-prop as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Layer(A_in, W, B, g):\n",
    "    \"\"\"\n",
    "    :param A_in: shape(m,n) - input data\n",
    "    :param W: shape(feature,units) - weight matrix, n0 feature  x units, \n",
    "    :param b: shape(units,1) - bias vector, n0 units x 1\n",
    "    :param g: activation function(e.g sigmoid, relu, softmax, ...)\n",
    "    :return:\n",
    "    A_out: shape(m, units): output data - m x units\n",
    "    \"\"\"\n",
    "    Z = np.dot(A_in, W) + B\n",
    "    A_out = g(Z)\n",
    "    return Z,A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_prop(x, W1, b1, W2, b2):\n",
    "    z1, a1 = Layer(x, W1, b1, sigmoid)  #hidden layer with sigmoid activation\n",
    "    z2, a2 = Layer(a1, W2, b2, softmax) #output layer with softmax activation\n",
    "    return z1, a1, z2, a2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model parameters\n",
    "The model parameters are intialized randomly as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_model_params():\n",
    "    W1 = np.random.randn(n, hidden_layer_units)\n",
    "    b1 = np.random.randn(hidden_layer_units)\n",
    "    W2 = np.random.randn(hidden_layer_units, num_class)\n",
    "    b2 = np.random.randn(num_class)\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x): \n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def compute_backward_prop(Z1, A1, A2, W2,X, Y): \n",
    "    dz2 = A2 - Y\n",
    "    dW2 = np.dot(A1.T,dz2)\n",
    "    db2 = dz2\n",
    "    dz1 = np.dot(dz2,W2.T)\n",
    "    db1 = dz1 * sigmoid_derivative(Z1)\n",
    "    dW1 = np.dot(X.T,dz1 * sigmoid_derivative(Z1))\n",
    "    return db1, dW1, db2, dW2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha_):\n",
    "    W1 = W1 - alpha_ * dW1\n",
    "    W2 = W2 - alpha_ * dW2\n",
    "    b1 = b1 - alpha_ * db1.sum(axis=0)\n",
    "    b2 = b2 - alpha_ * db2.sum(axis=0)\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y_hat, y):\n",
    "    return np.mean(np.square(y_hat - y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction, accuracy and one-hot encode\n",
    "In order to analyze the results, the implementation of prediction and accuracy are described below:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_output_layer(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def predict(X, B1, W1, B2, W2):\n",
    "    Z1 = np.dot(X, W1) + B1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + B2\n",
    "    A2 = softmax(Z2)\n",
    "    prediction = np.argmax(A2, 0)\n",
    "    return prediction\n",
    "\n",
    "def compute_accuracy(Y_hat, Y):\n",
    "    correct_count = sum((Y[i] == Y_hat[i]).all() for i in range(len(Y)))\n",
    "    accuracy = correct_count / len(Y)\n",
    "    return accuracy\n",
    "\n",
    "def convert_to_one_hot(A):\n",
    "    num_classes = A.shape[1]\n",
    "    max_value = np.max(A) + 1\n",
    "    one_hot_encoded = np.zeros((A.shape[0], num_classes), dtype=int)\n",
    "    indices = np.argmax(A, axis=1)\n",
    "    one_hot_encoded[np.arange(A.shape[0]), indices] = 1\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap everything up  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(X, Y, X_v, Y_v, epochs, alpha):\n",
    "    b1, W1, b2, W2 = initialize_model_params()\n",
    "    for i in range(epochs):\n",
    "        z1, a1, z2, a2 = compute_forward_prop(X, W1, b1, W2, b2)\n",
    "        db1, dW1, db2, dW2 = compute_backward_prop(z1, a1, a2, W2, X, Y) \n",
    "        b1, W1, b2, W2 = update_model_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "\n",
    "        cost = compute_cost(a2, Y)\n",
    "        #avoid overfitting \n",
    "        _,_,_,y_hat = compute_forward_prop(X_v, W1, b1, W2, b2)\n",
    "        y_hat_encoded = convert_to_one_hot(y_hat)\n",
    "        accuracy = compute_accuracy(y_hat_encoded, Y_v)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            print(f\"cost = {cost}  accuracy={accuracy * 100}\")\n",
    "        # print(f\"Accuracy: {accuracy(convert_to_one_hot(a2), Y)}\")\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training process with $epochs=150$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "cost = 0.3204854569629083  accuracy=40.537265198949704\n",
      "Iteration:  10\n",
      "cost = 0.37141426065680894  accuracy=23.20743284185013\n",
      "Iteration:  20\n",
      "cost = 0.2488774359580971  accuracy=44.29408200363562\n",
      "Iteration:  30\n",
      "cost = 0.18221521111925298  accuracy=70.63219551605737\n",
      "Iteration:  40\n",
      "cost = 0.12435261959095559  accuracy=70.6523934558675\n",
      "Iteration:  50\n",
      "cost = 0.13528077211669987  accuracy=71.1169460715007\n",
      "Iteration:  60\n",
      "cost = 0.030364072847876625  accuracy=91.55726115936174\n",
      "Iteration:  70\n",
      "cost = 0.03971068332376786  accuracy=92.68834578872955\n",
      "Iteration:  80\n",
      "cost = 0.02438133269940144  accuracy=94.64754595031307\n",
      "Iteration:  90\n",
      "cost = 0.025728821849999483  accuracy=93.49626338113512\n",
      "Iteration:  100\n",
      "cost = 0.02354410228287669  accuracy=93.75883659866695\n",
      "Iteration:  110\n",
      "cost = 0.022973567812959717  accuracy=93.83962835790749\n",
      "Iteration:  120\n",
      "cost = 0.022233967364524355  accuracy=94.04160775600889\n",
      "Iteration:  130\n",
      "cost = 0.019935447156864874  accuracy=95.55645324176933\n",
      "Iteration:  140\n",
      "cost = 0.019247927476592416  accuracy=94.52635831145223\n",
      "Iteration:  150\n",
      "cost = 0.019115202393230793  accuracy=95.83922439911129\n",
      "Iteration:  160\n",
      "cost = 0.016552201983901005  accuracy=95.79882851949101\n",
      "Iteration:  170\n",
      "cost = 0.017689707131667867  accuracy=94.62734801050293\n",
      "Iteration:  180\n",
      "cost = 0.035056421367176  accuracy=94.00121187638861\n",
      "Iteration:  190\n",
      "cost = 0.020682945163963178  accuracy=94.10220157543931\n"
     ]
    }
   ],
   "source": [
    "b1, W1, b2, W2 = training_data(X_train, Y_train, X_Validate, Y_Validate, epochs, alpha)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the proper arguments for out the model. Let's try some unseen tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = np.random.rand(1, n)\n",
    "# prediction = predict(X_test, b1, W1, b2, W2)\n",
    "# print(f\"prediction = {prediction}\")\n",
    "# print(f\"W1 = {W1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
