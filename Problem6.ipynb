{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from utils.activations import softmax, sigmoid\n",
    "from utils.data_parser import data_split_train_test\n",
    "np.random.seed(1)\n",
    "\n",
    "#define constants\n",
    "num_class = 4 # no of classes\n",
    "hidden_layer_units = 25  #number of units in the hidden layer\n",
    "epochs = 200  #number of iterations\n",
    "alpha = 0.001 #learning rate \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSource\n",
    "We split the provided data into 2 sets: training set and validation set with the ration 8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(os.path.join('data','train_data.csv'), header=None).add_prefix('Feature_')\n",
    "y = pd.read_csv(os.path.join('data', 'train_labels.csv'), header=None, names=[\"Label_0\", \"Label_1\", \"Label_2\", \"Label_3\"])\n",
    "X, Y, X_v, Y_v = data_split_train_test(x, y)\n",
    "X_train = X.values   #training samples\n",
    "Y_train = Y.values   #training labels\n",
    "\n",
    "m, n = X_train.shape  #samples x features\n",
    "X_Validate = X_v.values  #validation samples\n",
    "Y_Validate = Y_v.values  #validation labels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer, Forward Propagation, and Backward Propagation\n",
    "We then implement Layer, Forward Propagation, and Back Propagation as follow\n",
    "<img align=\"left\" src=\"./system.png\"     style=\" width:380px; padding: 10px; \" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Layer(A_in, W, B, g):\n",
    "    \"\"\"\n",
    "    :param A_in: shape(m,n) - input data\n",
    "    :param W: shape(feature,units) - weight matrix, n0 feature  x units, \n",
    "    :param b: shape(units,1) - bias vector, n0 units x 1\n",
    "    :param g: activation function(e.g sigmoid, relu, softmax, ...)\n",
    "    :return:\n",
    "    Z - linear regression\n",
    "    A_out: shape(m, units): output data - m x units\n",
    "    \"\"\"\n",
    "    Z = np.dot(A_in, W) + B\n",
    "    A_out = g(Z)\n",
    "    return Z,A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_prop(x, W1, b1, W2, b2):\n",
    "    z1, a1 = Layer(x, W1, b1, sigmoid)  #hidden layer [1] with sigmoid activation\n",
    "    z2, a2 = Layer(a1, W2, b2, softmax) #output layer [2] with softmax activation\n",
    "    return z1, a1, z2, a2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back-propagation is computed by the formulas:\n",
    "$$dz^{[2]}=a^{[2]} - Ytrain$$\n",
    "$$dW^{[2]}=a^{[1]T}\\cdot dz^{[2]}$$\n",
    "$$db^{[2]}=dz^{[2]}$$\n",
    "$$dz^{[1]}=dz^{[2]}  W^{[2]T}$$\n",
    "$$dW^{[1]}=X^{T}\\cdot dz^{[1]}*(z^{[1]})^{'}$$\n",
    "$$db^{[1]}=dz^{[1]}*(z^{[1]})^{'}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x): \n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def compute_backward_prop(Z1, A1, A2, W2,X, Y): \n",
    "    dz2 = A2 - Y\n",
    "    dW2 = np.dot(A1.T,dz2)\n",
    "    db2 = dz2\n",
    "    dz1 = np.dot(dz2,W2.T)\n",
    "    db1 = dz1 * sigmoid_derivative(Z1)\n",
    "    dW1 = np.dot(X.T,dz1 * sigmoid_derivative(Z1))\n",
    "    return db1, dW1, db2, dW2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent \n",
    "Then, these values from back-prop will be fed into gradient descent algorithm to re-calculate the weights and bias, so that the system can minimize the cost function:\n",
    "$$W^{[1]} = W^{[1]} - \\alpha dW^{[1]}$$\n",
    "$$b^{[1]} = b^{[1]} - \\alpha db^{[1]}$$\n",
    "$$W^{[2]} = W^{[2]} - \\alpha dW^{[2]}$$\n",
    "$$b^{[2]} = b^{[2]} - \\alpha db^{[2]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha_):\n",
    "    W1 = W1 - alpha_ * dW1\n",
    "    W2 = W2 - alpha_ * dW2\n",
    "    b1 = b1 - alpha_ * db1.sum(axis=0)\n",
    "    b2 = b2 - alpha_ * db2.sum(axis=0)\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "Cost function is defined by the average of square of error function\n",
    "$$Cost = \\frac{1}{m}\\sum_{i=0}^{m}(\\hat{y}-y)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y_hat, y):\n",
    "    return np.mean(np.square(y_hat - y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model parameters\n",
    "The model parameters are intialized randomly as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_model_params():\n",
    "    W1 = np.random.randn(n, hidden_layer_units)\n",
    "    b1 = np.random.randn(hidden_layer_units)\n",
    "    W2 = np.random.randn(hidden_layer_units, num_class)\n",
    "    b2 = np.random.randn(num_class)\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and one-hot encode\n",
    "In order to analyze the results, the implementation of prediction and accuracy are showed below:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, B1, W1, B2, W2):\n",
    "    Z1 = np.dot(X, W1) + B1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + B2\n",
    "    A2 = softmax(Z2)\n",
    "    prediction = np.argmax(A2, 0)\n",
    "    return prediction\n",
    "\n",
    "def compute_accuracy(Y_hat, Y):\n",
    "    correct_count = sum((Y[i] == Y_hat[i]).all() for i in range(len(Y)))\n",
    "    accuracy = correct_count / len(Y)\n",
    "    return accuracy\n",
    "\n",
    "def convert_to_one_hot(A):\n",
    "    num_classes = A.shape[1]\n",
    "    max_value = np.max(A) + 1\n",
    "    one_hot_encoded = np.zeros((A.shape[0], num_classes), dtype=int)\n",
    "    indices = np.argmax(A, axis=1)\n",
    "    one_hot_encoded[np.arange(A.shape[0]), indices] = 1\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap everything up \n",
    "Sequence of training process: Compute forward prop -> compute backward prop -> compute gradient descend, then repeat until convergence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(X, Y, X_v, Y_v, epochs, alpha):\n",
    "    b1, W1, b2, W2 = initialize_model_params()\n",
    "    accuracy = 0.\n",
    "    for i in range(epochs):\n",
    "        z1, a1, z2, a2 = compute_forward_prop(X, W1, b1, W2, b2)\n",
    "        db1, dW1, db2, dW2 = compute_backward_prop(z1, a1, a2, W2, X, Y) \n",
    "        b1, W1, b2, W2 = update_model_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "\n",
    "        #compute cost for each epoch        \n",
    "        cost = compute_cost(a2, Y) \n",
    "        #avoid overfitting by using validation set for prediction\n",
    "        _,_,_,y_hat = compute_forward_prop(X_v, W1, b1, W2, b2)\n",
    "        #convert prediction to one-hot encode\n",
    "        y_hat_encoded = convert_to_one_hot(y_hat)\n",
    "\n",
    "        accuracy = compute_accuracy(y_hat_encoded, Y_v)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Epoch: \", i)\n",
    "            print(f\"cost = {cost}  accuracy={accuracy * 100}\")\n",
    "    return b1, W1, b2, W2, accuracy * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training process with $epochs=200, \\alpha=0.001$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "cost = 0.3204854569629083  accuracy=40.537265198949704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quanquach/Desktop/ECE657/Assignment1/utils/activations.py:21: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10\n",
      "cost = 0.37141426065680894  accuracy=23.20743284185013\n",
      "Epoch:  20\n",
      "cost = 0.2488774359580971  accuracy=44.29408200363562\n",
      "Epoch:  30\n",
      "cost = 0.18221521111925298  accuracy=70.63219551605737\n",
      "Epoch:  40\n",
      "cost = 0.12435261959095559  accuracy=70.6523934558675\n",
      "Epoch:  50\n",
      "cost = 0.13528077211669987  accuracy=71.1169460715007\n",
      "Epoch:  60\n",
      "cost = 0.030364072847876625  accuracy=91.55726115936174\n",
      "Epoch:  70\n",
      "cost = 0.03971068332376786  accuracy=92.68834578872955\n",
      "Epoch:  80\n",
      "cost = 0.02438133269940144  accuracy=94.64754595031307\n",
      "Epoch:  90\n",
      "cost = 0.025728821849999483  accuracy=93.49626338113512\n",
      "Epoch:  100\n",
      "cost = 0.02354410228287669  accuracy=93.75883659866695\n",
      "Epoch:  110\n",
      "cost = 0.022973567812959717  accuracy=93.83962835790749\n",
      "Epoch:  120\n",
      "cost = 0.022233967364524355  accuracy=94.04160775600889\n",
      "Epoch:  130\n",
      "cost = 0.019935447156864874  accuracy=95.55645324176933\n",
      "Epoch:  140\n",
      "cost = 0.019247927476592416  accuracy=94.52635831145223\n",
      "Epoch:  150\n",
      "cost = 0.019115202393230793  accuracy=95.83922439911129\n",
      "Epoch:  160\n",
      "cost = 0.016552201983901005  accuracy=95.79882851949101\n",
      "Epoch:  170\n",
      "cost = 0.017689707131667867  accuracy=94.62734801050293\n",
      "Epoch:  180\n",
      "cost = 0.035056421367176  accuracy=94.00121187638861\n",
      "Epoch:  190\n",
      "cost = 0.020682945163963178  accuracy=94.10220157543931\n"
     ]
    }
   ],
   "source": [
    "b1, W1, b2, W2, accuracy = training_data(X_train, Y_train, X_Validate, Y_Validate, epochs, alpha)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the validation set is around 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.05150474651586"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the model parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -1.20879954,  -2.31890563,  -5.69133798,  -4.1128773 ,\n",
       "         -0.50367965,  -2.05513728,  -1.14467325,  -2.42138646,\n",
       "         -4.43622852,   0.63476919,  -4.8365037 ,  -3.62261245,\n",
       "          0.70081257,  -5.42958767, -10.37903416,  -3.8499326 ,\n",
       "         -2.93411032,  -3.57072241,  -6.13875778,  -2.88279477,\n",
       "         -1.91591634,  -0.26224121,   0.11872386,  -0.05141358,\n",
       "         -1.73938463]),\n",
       " array([[ 1.62434536, -0.61175641, -0.52817175, ...,  0.90159072,\n",
       "          0.50249434,  0.90085595],\n",
       "        [-0.68372786, -0.12289023, -0.93576943, ...,  2.10025514,\n",
       "          0.12015895,  0.61720311],\n",
       "        [ 0.30017032, -0.35224985, -1.1425182 , ...,  0.16003707,\n",
       "          0.87616892,  0.31563495],\n",
       "        ...,\n",
       "        [-0.32425465,  1.06686631, -0.37814658, ...,  0.3938322 ,\n",
       "         -0.15137905, -0.41505414],\n",
       "        [-0.85782245,  0.47026103, -0.71104247, ..., -0.91428204,\n",
       "          0.80115669,  0.11465216],\n",
       "        [-2.30445222,  1.69949757, -0.76992763, ..., -0.51091193,\n",
       "         -0.53900317,  0.86116192]]),\n",
       " array([-0.26656521,  2.53550063,  1.71925644, -2.14940234]),\n",
       " array([[ -0.14806992,  -6.30914277,   3.23496857,  -0.18679959],\n",
       "        [ -5.60182419,   5.65408734,  -3.24935791,   3.93956282],\n",
       "        [  9.70349961, -12.47072671,   4.08036286,  -3.2960904 ],\n",
       "        [  9.10968745,  -0.35905069,  -6.99727148,   0.73357373],\n",
       "        [  3.51244706,  -3.91064303,   2.35093091,  -0.58915803],\n",
       "        [ -0.14858093,  -1.67120603,   2.57638893,  -3.3451764 ],\n",
       "        [  2.23074112,  -4.90111387,   6.04718634,  -4.88910126],\n",
       "        [  3.32714786,  -5.54462736,   5.55110101,  -4.73728988],\n",
       "        [  5.88778811,   0.04381583,  -5.50470092,  -0.34181693],\n",
       "        [ -2.09374796,  -2.38887068,   1.52824248,   0.52782574],\n",
       "        [ -0.84464531,   1.79774072,  -4.62851526,   6.06991337],\n",
       "        [  4.48999478,  -0.54048181,  -8.45704881,   2.09840448],\n",
       "        [  1.13862147,   5.61297825,   0.33053823,  -5.22733797],\n",
       "        [ -3.91480094,  -0.65108727,   1.09193566,   6.23155271],\n",
       "        [  8.70626504,  -4.17923366,  -4.32393916,  -2.46492558],\n",
       "        [ 11.79670806,  -5.50516819,  -2.38518624,  -2.46166324],\n",
       "        [  3.00619855,   0.79025123,   2.30335219,  -5.4372753 ],\n",
       "        [  4.53650142,  -3.28376243,   4.0127549 ,  -7.84791448],\n",
       "        [  8.96579323,  -5.67282596,  -2.27463854,   0.67376984],\n",
       "        [ 10.8150524 ,  -8.52860221,  -1.77883659,  -0.72707178],\n",
       "        [ -0.23843217,  22.5121339 , -10.24053393, -11.53670418],\n",
       "        [ -1.63653381,  -1.25888336,   0.7563123 ,  -2.05874291],\n",
       "        [ -0.09298801,   0.36123429,   1.53821929,  -0.09228893],\n",
       "        [ -1.44445446,  -1.01226897,   0.46346584,   1.05042254],\n",
       "        [  1.28594929,  -0.60860925, -12.47593711,  10.48824519]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save them for later prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hidden_weight.npy', W1)\n",
    "np.save('hidden_bias.npy', b1)\n",
    "np.save('output_weight.npy', W2)\n",
    "np.save('output_bias.npy', b2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = np.random.rand(1, n)\n",
    "# print(X_test.shape)\n",
    "# prediction = predict(X_test, b1, W1, b2, W2)\n",
    "# print(f\"prediction = {prediction}\")\n",
    "# print(f\"X_test = {X_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
