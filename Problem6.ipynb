{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from utils.activations import softmax, relu\n",
    "\n",
    "sns.set()\n",
    "\n",
    "#define constants\n",
    "m = 24754  # no of examples\n",
    "n = 784 # no of features \n",
    "num_class = 4 # no of classes\n",
    "hidden_layer_units = 10 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSource\n",
    "X_train, Y_train is parsed from csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(os.path.join('data','train_data.csv'), header=None).add_prefix('Feature_')\n",
    "Y_train = pd.read_csv(os.path.join('data', 'train_labels.csv'), header=None, names=[\"Label_0\", \"Label_1\", \"Label_2\", \"Label_3\"])\n",
    "X_train_transpose = X_train.T\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer and Forward Propagation\n",
    "We then implement Layer and For-prop as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Layer(A_in, W, B, g):\n",
    "    \"\"\"\n",
    "    :param A_in: shape(m,n) - input data\n",
    "    :param W: shape(feature,units) - weight matrix, n0 feature  x units, \n",
    "    :param b: shape(units,1) - bias vector, n0 units x 1\n",
    "    :param g: activation function(e.g sigmoid, relu, softmax, ...)\n",
    "    :return:\n",
    "    A_out: shape(m, units): output data - m x units\n",
    "    \"\"\"\n",
    "    Z = np.matmul(A_in, W) + B\n",
    "    A_out = g(Z)\n",
    "    return Z,A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sequence(x, W1, b1, W2, b2):\n",
    "    z1, a1 = Layer(x, W1, b1, relu)  #hidden layer with relu activation\n",
    "    z2, a2 = Layer(a1, W2, b2, softmax) #output layer with softmax activation\n",
    "    return z1, a1, z2, a2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model parameters\n",
    "The model parameters are intialized randomly as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_model_params():\n",
    "    W1 = np.random.rand(n, hidden_layer_units)\n",
    "    B1 = np.random.rand(1, hidden_layer_units)\n",
    "    W2 = np.random.rand(hidden_layer_units, num_class)\n",
    "    B2 = np.random.rand(1, num_class)\n",
    "    return B1, W1, B2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x): \n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def compute_back_prop(Z1, Z2, A1, A2, W2,X, Y):\n",
    "    m = Y.shape[0]\n",
    "    dz2 = A2 - Y\n",
    "    print(f\"A2 shape={A2.shape}\")\n",
    "    print(f\"Y shape={Y.shape}\")\n",
    "    print(f\"dz2 shape={(A2 - Y).shape}\")\n",
    "    print(f\"A1 shape={A1.T.shape}\")\n",
    "    dW2 = (1/m) * np.matmul(A1.T,dz2)\n",
    "    db2 = (1/m)* np.sum(dz2, axis=0)\n",
    "    dz1 = np.matmul(W2.T, dz2) * relu_derivative(A1)\n",
    "    db1 = (1/m) * np.sum(dz1, axis=0)\n",
    "    dW1 = (1/m) * np.matmul(X.T,dz1)\n",
    "    return db1, dW1, db2, dW2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_descent(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha_):\n",
    "    W1 = W1 - alpha_ * dW1\n",
    "    b1 = b1 - alpha_ * db1\n",
    "    W2 = W2 - alpha_ * dW2\n",
    "    b2 = b2 - alpha_ * db2\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap everything up  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(X, Y, epochs, alpha):\n",
    "    B1, W1, B2, W2 = initialize_model_params()\n",
    "    for i in range(epochs):\n",
    "        z1, a1, z2, a2 = Sequence(X, W1, B1, W2, B2)   \n",
    "        db1, dW1, db2, dW2 = compute_back_prop(z1, z2, a1, a2, W2, X, Y)\n",
    "        b1, W1, b2, W2 = compute_gradient_descent(W1, B1, W2, B2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m b1, W1, b2, W2 \u001b[39m=\u001b[39m training_data(X_train, Y_train, \u001b[39m1\u001b[39;49m, \u001b[39m0.01\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m, in \u001b[0;36mtraining_data\u001b[0;34m(X, Y, epochs, alpha)\u001b[0m\n\u001b[1;32m      2\u001b[0m B1, W1, B2, W2 \u001b[39m=\u001b[39m initialize_model_params()\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m----> 4\u001b[0m     z1, a1, z2, a2 \u001b[39m=\u001b[39m Sequence(X, W1, B1, W2, B2)   \n\u001b[1;32m      5\u001b[0m     db1, dW1, db2, dW2 \u001b[39m=\u001b[39m compute_back_prop(z1, z2, a1, a2, W2, X, Y)\n\u001b[1;32m      6\u001b[0m     b1, W1, b2, W2 \u001b[39m=\u001b[39m compute_gradient_descent(W1, B1, W2, B2, dW1, db1, dW2, db2, alpha)\n",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m, in \u001b[0;36mSequence\u001b[0;34m(x, W1, b1, W2, b2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mSequence\u001b[39m(x, W1, b1, W2, b2):\n\u001b[1;32m      2\u001b[0m     z1, a1 \u001b[39m=\u001b[39m Layer(x, W1, b1, relu)  \u001b[39m#hidden layer with relu activation\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     z2, a2 \u001b[39m=\u001b[39m Layer(a1, W2, b2, softmax) \u001b[39m#output layer with softmax activation\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m z1, a1, z2, a2\n",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m, in \u001b[0;36mLayer\u001b[0;34m(A_in, W, B, g)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m:param A_in: shape(m,n) - input data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m:param W: shape(feature,units) - weight matrix, n0 feature  x units, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mA_out: shape(m, units): output data - m x units\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m Z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(A_in, W) \u001b[39m+\u001b[39m B\n\u001b[0;32m---> 11\u001b[0m A_out \u001b[39m=\u001b[39m g(Z)\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m Z,A_out\n",
      "File \u001b[0;32m~/Desktop/ECE657/Assignment1/utils/activations.py:44\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mSoftmax function for a matrix or vector X\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mParams:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m    softmax output as a numpy array\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# exp_X = np.exp(X)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# sum_exp_X = np.sum(exp_X, axis=-1, keepdims=True)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# softmax_output = exp_X / sum_exp_X\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m ez \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(z)              \u001b[39m#element-wise exponenial\u001b[39;00m\n\u001b[1;32m     45\u001b[0m softmax_output \u001b[39m=\u001b[39m ez\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39msum(ez)\n\u001b[1;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m softmax_output\n",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "b1, W1, b2, W2 = training_data(X_train, Y_train, 1, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
