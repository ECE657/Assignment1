{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from utils.activations import softmax, relu\n",
    "\n",
    "sns.set()\n",
    "\n",
    "#define constants\n",
    "m = 24754  # no of examples\n",
    "n = 784 # no of features \n",
    "num_class = 4 # no of classes\n",
    "hidden_layer_units = 10 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSource\n",
    "X_train, Y_train is parsed from csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(os.path.join('data','train_data.csv'), header=None).add_prefix('Feature_')\n",
    "y = pd.read_csv(os.path.join('data', 'train_labels.csv'), header=None, names=[\"Label_0\", \"Label_1\", \"Label_2\", \"Label_3\"])\n",
    "X_train = x.to_numpy()\n",
    "Y_train = y.to_numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer and Forward Propagation\n",
    "We then implement Layer and For-prop as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Layer(A_in, W, B, g):\n",
    "    \"\"\"\n",
    "    :param A_in: shape(m,n) - input data\n",
    "    :param W: shape(feature,units) - weight matrix, n0 feature  x units, \n",
    "    :param b: shape(units,1) - bias vector, n0 units x 1\n",
    "    :param g: activation function(e.g sigmoid, relu, softmax, ...)\n",
    "    :return:\n",
    "    A_out: shape(m, units): output data - m x units\n",
    "    \"\"\"\n",
    "    Z = np.matmul(A_in, W) + B\n",
    "    A_out = g(Z)\n",
    "    return Z,A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sequence(x, W1, b1, W2, b2):\n",
    "    z1, a1 = Layer(x, W1, b1, relu)  #hidden layer with relu activation\n",
    "    z2, a2 = Layer(a1, W2, b2, softmax) #output layer with softmax activation\n",
    "    return z1, a1, z2, a2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model parameters\n",
    "The model parameters are intialized randomly as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_model_params():\n",
    "    W1 = np.random.rand(n, hidden_layer_units)\n",
    "    B1 = np.random.rand(1, hidden_layer_units)\n",
    "    W2 = np.random.rand(hidden_layer_units, num_class)\n",
    "    B2 = np.random.rand(1, num_class)\n",
    "    return B1, W1, B2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x): \n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def compute_back_prop(Z1, Z2, A1, A2, W2,X, Y):\n",
    "    m = Y.shape[0]\n",
    "    dz2 = A2 - Y\n",
    "    dW2 = (1/m) * np.matmul(A1.T,dz2)\n",
    "    db2 = (1/m)* np.sum(dz2, axis=0)\n",
    "    dz1 = np.matmul(dz2,W2.T) * relu_derivative(Z1)\n",
    "    db1 = (1/m) * np.sum(dz1, axis=0)\n",
    "    dW1 = (1/m) * np.matmul(X.T,dz1)\n",
    "    return db1, dW1, db2, dW2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_descent(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha_):\n",
    "    W1 = W1 - alpha_ * dW1\n",
    "    b1 = b1 - alpha_ * db1\n",
    "    W2 = W2 - alpha_ * dW2\n",
    "    b2 = b2 - alpha_ * db2\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and accuracy\n",
    "In order to analyze the results, the implementation of prediction and accuracy are described below:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_output_layer(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def predict(X, B1, W1, B2, W2):\n",
    "    Z1 = np.matmul(X, W1) + B1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.matmul(A1, W2) + B2\n",
    "    A2 = softmax(Z2)\n",
    "    prediction = np.argmax(A2, 0)\n",
    "    return prediction\n",
    "\n",
    "def accuracy(Y_hat, Y):\n",
    "    print(Y_hat, Y)\n",
    "    return np.sum(Y_hat == Y) / Y.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap everything up  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(X, Y, epochs, alpha):\n",
    "    B1, W1, B2, W2 = initialize_model_params()\n",
    "    for i in range(epochs):\n",
    "        z1, a1, z2, a2 = Sequence(X, W1, B1, W2, B2)   \n",
    "        db1, dW1, db2, dW2 = compute_back_prop(z1, z2, a1, a2, W2, X, Y)\n",
    "        b1, W1, b2, W2 = compute_gradient_descent(W1, B1, W2, B2, dW1, db1, dW2, db2, alpha)\n",
    "        # if i % 50 == 0:\n",
    "            # print(\"Iteration: \", i)\n",
    "            # print(\"Accuracy: \", accuracy(predict_from_output_layer(a2), Y))\n",
    "    return b1, W1, b2, W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training process with $epochs=1000$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "b1, W1, b2, W2 = training_data(X_train, Y_train, 100, 0.1)\n",
    "print(W2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the proper arguments for out the model. Let's try some unseen tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction = [0 0 0 0]\n",
      "W1 = [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.random.rand(1, n)\n",
    "prediction = predict(X_test, b1, W1, b2, W2)\n",
    "print(f\"prediction = {prediction}\")\n",
    "print(f\"W1 = {W1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
